{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Models import modelpool\n",
    "from Preprocess import datapool\n",
    "from torch import nn\n",
    "from spiking_layer_ours import SPIKE_layer\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from utils_my import add_dimension\n",
    "from utils_my import replace_maxpool2d_by_avgpool2d, replace_layer_by_tdlayer\n",
    "\n",
    "def isActivation(name):\n",
    "    if 'relu' in name.lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def replace_activation_by_spike(model, thresholds, thresholds1, n_steps, counter=0):\n",
    "    thresholds_new = deepcopy(thresholds)\n",
    "    thresholds_new1 = deepcopy(thresholds1)\n",
    "    \n",
    "    for name, module in model._modules.items():\n",
    "        if hasattr(module,\"_modules\"):\n",
    "            model._modules[name], counter = replace_activation_by_spike(module, thresholds_new, thresholds_new1, n_steps, counter)\n",
    "        if isActivation(module.__class__.__name__.lower()):\n",
    "            thresholds_new[counter, n_steps:] = thresholds_new1[counter,1] / n_steps  # thresholds_out_sum/n_steps# thresholds1[counter,1] / n_steps\n",
    "            thresholds_new[counter, :n_steps] = thresholds_new1[counter,0] / n_steps  # thresholds_inner_sum/n_steps#thresholds1[counter,0] / n_steps\n",
    "            model._modules[name] = SPIKE_layer(thresholds_new[counter, n_steps:], thresholds_new[counter, 0:n_steps])\n",
    "            counter += 1\n",
    "    return model, counter\n",
    "\n",
    "\n",
    "def interpolate_state_dicts(state_dict_1, state_dict_2, weight,\n",
    "                            bias_norm=False):\n",
    "    if not bias_norm:\n",
    "        return {key: (1 - weight) * state_dict_1[key] +\n",
    "                weight * state_dict_2[key] for key in state_dict_1.keys()}\n",
    "    else:\n",
    "        model_state = deepcopy(state_dict_1)\n",
    "        height = 0\n",
    "        for p_name in model_state:\n",
    "            if \"batches\" not in p_name:\n",
    "                model_state[p_name].zero_()\n",
    "                if \"weight\" in p_name:\n",
    "                    model_state[p_name].add_(1.0 - weight, state_dict_1[p_name])\n",
    "                    model_state[p_name].add_(weight, state_dict_2[p_name])\n",
    "                    height += 1\n",
    "                if \"bias\" in p_name:\n",
    "                    model_state[p_name].add_((1.0 - weight)**height, state_dict_1[p_name])\n",
    "                    model_state[p_name].add_(weight**height, state_dict_2[p_name])\n",
    "                if \"res_scale\" in p_name:\n",
    "                    model_state[p_name].add_(1.0 - weight, state_dict_1[p_name])\n",
    "                    model_state[p_name].add_(weight, state_dict_2[p_name])\n",
    "        return model_state\n",
    "\n",
    "def interpolate_multi_state_dicts(sd_s, weight_s):\n",
    "    sd_interpolated = deepcopy(sd_s[0])\n",
    "    for key in sd_s[0].keys():\n",
    "        sd_interpolated[key] = weight_s[0] * sd_s[0][key]\n",
    "        for i in range(1, len(sd_s)):\n",
    "            sd_interpolated[key] += weight_s[i] * sd_s[i][key]\n",
    "    return sd_interpolated\n",
    "    \n",
    "\n",
    "def validate_snn(model, loader, n_steps, thresholds, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data, target in loader:\n",
    "        data = add_dimension(data, n_steps)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data, thresholds, L=0, t=n_steps)\n",
    "        output = torch.mean(output, dim=1)\n",
    "        total += target.size(0)\n",
    "        correct += (output.argmax(1) == target).sum().item()\n",
    "    print('Accuracy of the network on the test images: %f' % (100 * correct / total))\n",
    "    acc = 100 * correct / total\n",
    "    return acc\n",
    "def validate_snn_ensemble(models, loader, n_steps, thresholds, device):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data, target in loader:\n",
    "        data = add_dimension(data, n_steps)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = torch.zeros(data.size(0), 10).to(device)\n",
    "        for model in models:\n",
    "            output += torch.mean(model(data, thresholds, L=0, t=n_steps), dim=1)\n",
    "        total += target.size(0)\n",
    "        correct += (output.argmax(1) == target).sum().item()\n",
    "    print('Accuracy of the network on the test images: %f' % (100 * correct / total))\n",
    "    acc = 100 * correct / total\n",
    "    return acc\n",
    "    \n",
    "def validate_ann(model, loader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        total += target.size(0)\n",
    "        correct += (output.argmax(1) == target).sum().item()\n",
    "    print('Accuracy of the network on the test images: %f' % (100 * correct / total))\n",
    "    acc = 100 * correct / total\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def ann_to_snn(model, thresholds, thresholds1, n_steps):\n",
    "    model, _ = replace_activation_by_spike(model, thresholds, thresholds1, n_steps)\n",
    "    model = replace_maxpool2d_by_avgpool2d(model)\n",
    "    model = replace_layer_by_tdlayer(model)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class args:\n",
    "    model = 'vgg16'\n",
    "    dataset = 'cifar10'\n",
    "    batch_size = 128\n",
    "    t = 1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_steps = args.t\n",
    "train_loader, test_loader = datapool(args.dataset, args.batch_size, 0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2787783/1612391968.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(f'saved_models/cifar10_vgg16_{ckpt_model_idx}.pth')\n"
     ]
    }
   ],
   "source": [
    "model_s = [modelpool(args.model, args.dataset).to(device) for _ in range(3)]\n",
    "ckpt_model_idx = 3\n",
    "sd = torch.load(f'saved_models/cifar10_vgg16_{ckpt_model_idx}.pth')\n",
    "for model in model_s:\n",
    "    model.load_state_dict(sd)\n",
    "\n",
    "num_relu = str(model_s[0]).count('ReLU')\n",
    "threshold_all_noaug1 = np.load(f'cifar10_vgg16_{ckpt_model_idx}_threshold_all_noaug{n_steps}.npy')\n",
    "threshold_pos_all_noaug1 = np.load(f'cifar10_vgg16_{ckpt_model_idx}_threshold_pos_all_noaug{n_steps}.npy')\n",
    "thresholds = torch.zeros(num_relu,2*n_steps)\n",
    "thresholds1 = torch.Tensor(np.load(f'cifar10_vgg16_{ckpt_model_idx}_threshold_all_noaug{n_steps}.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 95.880000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95.88"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_ann(model_s[0], test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 45.690000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45.69"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s = [ann_to_snn(model, thresholds, thresholds1, n_steps) for model in model_s]\n",
    "validate_snn(model_s[0], test_loader, n_steps, thresholds, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2787783/2261157115.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd_s = [torch.load('cifar10_vgg16_0_updated_snn1_1.pth'),\n",
      "/tmp/ipykernel_2787783/2261157115.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load('cifar10_vgg16_0_updated_snn1_1_v2.pth'),\n",
      "/tmp/ipykernel_2787783/2261157115.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load('cifar10_vgg16_0_updated_snn1_1_v3.pth')]\n"
     ]
    }
   ],
   "source": [
    "# model_s = [ann_to_snn(model, thresholds, thresholds1, n_steps) for model in model_s]\n",
    "sd_s = [torch.load('cifar10_vgg16_0_updated_snn1_1.pth'),\n",
    "        torch.load('cifar10_vgg16_0_updated_snn1_1_v2.pth'),\n",
    "        torch.load('cifar10_vgg16_0_updated_snn1_1_v3.pth')]\n",
    "for model, sd in zip(model_s, sd_s):\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.330000\n",
      "Accuracy of the network on the test images: 92.210000\n",
      "Accuracy of the network on the test images: 92.040000\n",
      "Accuracy of the network on the test images: 93.860000\n",
      "[92.33, 92.21, 92.04] 93.86\n"
     ]
    }
   ],
   "source": [
    "acc_s = [validate_snn(model, test_loader, n_steps, thresholds, device) for model in model_s]\n",
    "acc_ensemble = validate_snn_ensemble(model_s, test_loader, n_steps, thresholds, device)\n",
    "print(acc_s, acc_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.500000\n"
     ]
    }
   ],
   "source": [
    "# bn_loader, _ = datapool(args.dataset, 512, 0, shuffle=False)\n",
    "bn_loader = train_loader\n",
    "model_mid = modelpool(args.model, args.dataset)\n",
    "model_mid = ann_to_snn(model_mid, thresholds, thresholds1, n_steps)\n",
    "merged_indices = [0, 2]\n",
    "merged_sds = [sd_s[i] for i in merged_indices]\n",
    "num_models = len(merged_sds)\n",
    "sd_mid = interpolate_multi_state_dicts(merged_sds, [1/num_models]*num_models)\n",
    "model_mid.load_state_dict(sd_mid)\n",
    "model_mid = model_mid.to(device)\n",
    "acc_mid = validate_snn(model_mid, test_loader, n_steps, thresholds, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.200000\n"
     ]
    }
   ],
   "source": [
    "def bn_calibration_init(m):\n",
    "    \"\"\" calculating post-statistics of batch normalization \"\"\"\n",
    "    if getattr(m, 'track_running_stats', False):\n",
    "        # reset all values for post-statistics\n",
    "        m.reset_running_stats()\n",
    "        # set bn in training mode to update post-statistics\n",
    "        m.training = True\n",
    "        # use cumulative moving average\n",
    "        m.momentum = None\n",
    "\n",
    "def reset_bn_stats(model, device, bn_loader, n_steps):\n",
    "    # Reset batch norm statistics\n",
    "    for m in model.modules():\n",
    "        bn_calibration_init(m)\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        for data, _ in bn_loader:\n",
    "            data = add_dimension(data, n_steps)\n",
    "            data = data.to(device)\n",
    "            model(data)\n",
    "    model.eval()\n",
    "\n",
    "model_mid.load_state_dict(sd_mid)\n",
    "reset_bn_stats(model_mid, device, bn_loader, n_steps)\n",
    "reset_acc_mid = validate_snn(model_mid, test_loader, n_steps, thresholds, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try fine-tuning to tune bn statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_snn(train_dataloader, test_dataloader, model, epochs, device, loss_fn, lr=0.1, wd=5e-4):\n",
    "#     model = model.to(device)\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=lr,momentum=0.9,weight_decay=wd) \n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs,verbose=True)\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0\n",
    "#         length = 0\n",
    "#         total = 0\n",
    "#         correct = 0\n",
    "#         for img, label in train_dataloader:\n",
    "#             img = add_dimension(img,n_steps)\n",
    "#             img = img.cuda(device)\n",
    "            \n",
    "#             labels = label.cuda(device)\n",
    "#             outputs = model(img,thresholds,L=0,t=n_steps) \n",
    "#             outputs = torch.sum(outputs,1)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = loss_fn(outputs/n_steps, labels)\n",
    "\n",
    "#             with torch.autograd.set_detect_anomaly(True):\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "                \n",
    "#             epoch_loss += loss.item()*img.shape[0]\n",
    "#             length += len(label)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#             #print(predicted)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             if total%(256*8) == 0:\n",
    "#                 print('Epoch:%d, Accuracy of the snn network on the %d train images: %f, loss:%f'%(epoch,total,100 * correct / total,epoch_loss/total))\n",
    "#         print('Epoch:%d, Accuracy of the snn network on the %d train images: %f, loss:%f'%(epoch,total,100 * correct / total,epoch_loss/total))\n",
    "#         scheduler.step()\n",
    "#         validate_snn(model, test_dataloader, n_steps, thresholds, device)\n",
    "# model_mid.load_state_dict(sd_mid) \n",
    "# train_snn(train_loader, test_loader, model_mid, 5, device, criterion, lr=1e-5, wd=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = np.load('cifar10_vgg16_0_threshold_all_noaug2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.load('cifar10_vgg16_0_updated_snn1_test_acc_1.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
