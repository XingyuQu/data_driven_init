{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Models import modelpool\n",
    "from Preprocess import datapool\n",
    "from torch import nn\n",
    "from spiking_layer_ours import SPIKE_layer\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from utils_my import add_dimension\n",
    "from utils_my import replace_maxpool2d_by_avgpool2d, replace_layer_by_tdlayer\n",
    "from slerp import slerp\n",
    "from collections import OrderedDict\n",
    "\n",
    "def isActivation(name):\n",
    "    if 'relu' in name.lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def replace_activation_by_spike(model, thresholds, thresholds1, n_steps, counter=0):\n",
    "    thresholds_new = deepcopy(thresholds)\n",
    "    thresholds_new1 = deepcopy(thresholds1)\n",
    "    \n",
    "    for name, module in model._modules.items():\n",
    "        if hasattr(module,\"_modules\"):\n",
    "            model._modules[name], counter = replace_activation_by_spike(module, thresholds_new, thresholds_new1, n_steps, counter)\n",
    "        if isActivation(module.__class__.__name__.lower()):\n",
    "            thresholds_new[counter, n_steps:] = thresholds_new1[counter,1] / n_steps  # thresholds_out_sum/n_steps# thresholds1[counter,1] / n_steps\n",
    "            thresholds_new[counter, :n_steps] = thresholds_new1[counter,0] / n_steps  # thresholds_inner_sum/n_steps#thresholds1[counter,0] / n_steps\n",
    "            model._modules[name] = SPIKE_layer(thresholds_new[counter, n_steps:], thresholds_new[counter, 0:n_steps])\n",
    "            counter += 1\n",
    "    return model, counter\n",
    "\n",
    "\n",
    "def interpolate_state_dicts(state_dict_1, state_dict_2, weight,\n",
    "                            bias_norm=False, use_slerp=False):\n",
    "    if use_slerp:\n",
    "        model_state = deepcopy(state_dict_1)\n",
    "        for p_name in model_state:\n",
    "            if \"batches\" not in p_name:\n",
    "                model_state[p_name] = slerp(weight, state_dict_1[p_name], state_dict_2[p_name])\n",
    "        return model_state\n",
    "    elif bias_norm:\n",
    "        model_state = deepcopy(state_dict_1)\n",
    "        height = 0\n",
    "        for p_name in model_state:\n",
    "            if \"batches\" not in p_name:\n",
    "                model_state[p_name].zero_()\n",
    "                if \"weight\" in p_name:\n",
    "                    model_state[p_name].add_(1.0 - weight, state_dict_1[p_name])\n",
    "                    model_state[p_name].add_(weight, state_dict_2[p_name])\n",
    "                    height += 1\n",
    "                if \"bias\" in p_name:\n",
    "                    model_state[p_name].add_((1.0 - weight)**height, state_dict_1[p_name])\n",
    "                    model_state[p_name].add_(weight**height, state_dict_2[p_name])\n",
    "                if \"res_scale\" in p_name:\n",
    "                    model_state[p_name].add_(1.0 - weight, state_dict_1[p_name])\n",
    "                    model_state[p_name].add_(weight, state_dict_2[p_name])\n",
    "        return model_state\n",
    "    else:\n",
    "        return {key: (1 - weight) * state_dict_1[key] + \n",
    "                weight * state_dict_2[key] for key in state_dict_1.keys()}\n",
    "        \n",
    "\n",
    "def interpolate_multi_state_dicts(sd_s, weight_s, use_slerp=False):\n",
    "    if use_slerp:\n",
    "        sd_interpolated = deepcopy(sd_s[0])\n",
    "        weight_interpolated = weight_s[0]\n",
    "        for i in range(1, len(sd_s)):\n",
    "            sd_next = sd_s[i]\n",
    "            weight_next = weight_s[i]\n",
    "            t = weight_next / (weight_interpolated + weight_next)\n",
    "            sd_interpolated = interpolate_state_dicts(sd_interpolated, sd_next, t, use_slerp=use_slerp)\n",
    "            weight_interpolated += weight_next\n",
    "        return sd_interpolated\n",
    "    else:\n",
    "        sd_interpolated = deepcopy(sd_s[0])\n",
    "        for key in sd_s[0].keys():\n",
    "            sd_interpolated[key] = weight_s[0] * sd_s[0][key]\n",
    "            for i in range(1, len(sd_s)):\n",
    "                sd_interpolated[key] += weight_s[i] * sd_s[i][key]\n",
    "        return sd_interpolated\n",
    "    \n",
    "\n",
    "def validate_snn(model, loader, n_steps, thresholds, device, verbose=1):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data, target in loader:\n",
    "        data = add_dimension(data, n_steps)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data, thresholds, L=0, t=n_steps)\n",
    "        output = torch.mean(output, dim=1)\n",
    "        total += target.size(0)\n",
    "        correct += (output.argmax(1) == target).sum().item()\n",
    "    if verbose != 0:\n",
    "        print('Accuracy of the network on the test images: %f' % (100 * correct / total))\n",
    "    acc = 100 * correct / total\n",
    "    model.to('cpu')\n",
    "    return acc\n",
    "\n",
    "def validate_snn_ensemble(models, loader, n_steps, thresholds, device, verbose=1):\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data, target in loader:\n",
    "        data = add_dimension(data, n_steps)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = torch.zeros(data.size(0), 10).to(device)\n",
    "        for model in models:\n",
    "            output += torch.mean(model(data, thresholds, L=0, t=n_steps), dim=1)\n",
    "        total += target.size(0)\n",
    "        correct += (output.argmax(1) == target).sum().item()\n",
    "    if verbose != 0:\n",
    "        print('Accuracy of the network on the test images: %f' % (100 * correct / total))\n",
    "    acc = 100 * correct / total\n",
    "    for model in models:\n",
    "        model.to('cpu')\n",
    "    return acc\n",
    "    \n",
    "def validate_ann(model, loader, device, verbose=1):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        total += target.size(0)\n",
    "        correct += (output.argmax(1) == target).sum().item()\n",
    "    if verbose != 0:\n",
    "        print('Accuracy of the network on the test images: %f' % (100 * correct / total))\n",
    "    acc = 100 * correct / total\n",
    "    model.to('cpu')\n",
    "    return acc\n",
    "\n",
    "\n",
    "def ann_to_snn(model, thresholds, thresholds1, n_steps):\n",
    "    model, _ = replace_activation_by_spike(model, thresholds, thresholds1, n_steps)\n",
    "    model = replace_maxpool2d_by_avgpool2d(model)\n",
    "    model = replace_layer_by_tdlayer(model)\n",
    "    return model\n",
    "\n",
    "def snn_to_ann(model, use_maxpooling=True):\n",
    "    new_model = deepcopy(model)\n",
    "    for name, module in new_model._modules.items():\n",
    "        if hasattr(module, \"_modules\"):\n",
    "            new_model._modules[name] = snn_to_ann(module)\n",
    "        if module.__class__.__name__ == 'SPIKE_layer':\n",
    "            new_model._modules[name] = nn.ReLU()\n",
    "        elif module.__class__.__name__ == 'tdLayer':\n",
    "            if use_maxpooling and isinstance(module.layer.module, nn.AvgPool2d):\n",
    "                module.layer.module = nn.MaxPool2d(kernel_size=module.layer.module.kernel_size,\n",
    "                                                stride=module.layer.module.stride,\n",
    "                                                padding=module.layer.module.padding)\n",
    "            new_model._modules[name] = module.layer.module\n",
    "        elif module.__class__.__name__ == 'Flatten':\n",
    "            new_model._modules[name] = nn.Flatten()\n",
    "    return new_model\n",
    "\n",
    "def tranfer_bn_stats_from_ann_to_snn(ann_model, snn_model):\n",
    "    new_snn_model = deepcopy(snn_model)\n",
    "    ann_bn_layer_names = [name for name, layer in ann_model.named_modules() if isinstance(layer, (nn.BatchNorm2d))]\n",
    "    snn_bn_layer_names = [name for name, layer in new_snn_model.named_modules() if isinstance(layer, (nn.BatchNorm2d))]\n",
    "    for ann_bn_name, snn_bn_name in zip(ann_bn_layer_names, snn_bn_layer_names):\n",
    "        ann_bn_layer = dict(ann_model.named_modules())[ann_bn_name]\n",
    "        snn_bn_layer = dict(new_snn_model.named_modules())[snn_bn_name]\n",
    "        snn_bn_layer.running_mean = ann_bn_layer.running_mean\n",
    "        snn_bn_layer.running_var = ann_bn_layer.running_var\n",
    "        snn_bn_layer.num_batches_tracked = ann_bn_layer.num_batches_tracked\n",
    "    return new_snn_model\n",
    "\n",
    "\n",
    "def bn_calibration_init(m):\n",
    "    \"\"\" calculating post-statistics of batch normalization \"\"\"\n",
    "    if getattr(m, 'track_running_stats', False):\n",
    "        # reset all values for post-statistics\n",
    "        m.reset_running_stats()\n",
    "        # set bn in training mode to update post-statistics\n",
    "        m.training = True\n",
    "        # use cumulative moving average\n",
    "        m.momentum = None\n",
    "\n",
    "def reset_bn_stats(model, device, bn_loader, n_steps, layerwise=False, ann=False):\n",
    "    # Reset batch norm statistics\n",
    "    model.to(device)\n",
    "    # get batchnorm layer names\n",
    "    bn_layer_names = [name for name, layer in model.named_modules() if isinstance(layer, (nn.BatchNorm2d))]\n",
    "    for m in model.modules():\n",
    "        bn_calibration_init(m)\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        L_range = range(1, 14) if layerwise else [0]\n",
    "        for L in L_range:\n",
    "            for data, _ in bn_loader:\n",
    "                if not ann:\n",
    "                    data = add_dimension(data, n_steps)\n",
    "                data = data.to(device)\n",
    "                model(data, thresholds=thresholds, L=L, t=n_steps)\n",
    "            # set current bn layer in eval mode\n",
    "            if layerwise:\n",
    "                layer = dict(model.named_modules())[bn_layer_names[L-1]]\n",
    "                layer.eval()\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "\n",
    "def get_device(item):\n",
    "    if isinstance(item, nn.Module):\n",
    "        return next(item.parameters()).device\n",
    "    elif isinstance(item, OrderedDict):\n",
    "        return next(iter(item.values())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class args:\n",
    "    model = 'vgg16'\n",
    "    dataset = 'cifar10'\n",
    "    batch_size = 128\n",
    "    t = 1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_steps = args.t\n",
    "train_loader, test_loader = datapool(args.dataset, args.batch_size, 0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_idx_s = list(range(2, 16)) + list(range(22, 37))\n",
    "# v_idx_s = [2]\n",
    "num_models = len(v_idx_s) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s = [modelpool(args.model, args.dataset) for _ in range(num_models)]\n",
    "ckpt_model_idx = 3\n",
    "sd = torch.load(f'saved_models/cifar10_vgg16_{ckpt_model_idx}.pth', map_location='cpu', weights_only=True)\n",
    "for model in model_s:\n",
    "    model.load_state_dict(sd)\n",
    "\n",
    "num_relu = str(model_s[0]).count('ReLU')\n",
    "prefix = 'cifar10_vgg16_3_lr_1e3/'\n",
    "threshold_all_noaug1 = np.load(f'{prefix}cifar10_vgg16_{ckpt_model_idx}_threshold_all_noaug{n_steps}.npy')\n",
    "threshold_pos_all_noaug1 = np.load(f'{prefix}cifar10_vgg16_{ckpt_model_idx}_threshold_pos_all_noaug{n_steps}.npy')\n",
    "thresholds = torch.zeros(num_relu,2*n_steps)\n",
    "thresholds1 = torch.Tensor(np.load(f'{prefix}cifar10_vgg16_{ckpt_model_idx}_threshold_all_noaug{n_steps}.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 95.880000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95.88"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_ann(model_s[0], test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 45.690000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45.69"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s = [ann_to_snn(model, thresholds, thresholds1, n_steps) for model in model_s]\n",
    "validate_snn(model_s[0], test_loader, n_steps, thresholds, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for case study\n",
    "# model_s[0].load_state_dict(torch.load(f'cifar10_vgg16_3_updated_snn1_1_v2.pth', map_location='cpu'))\n",
    "# validate_snn(model_s[0], test_loader, n_steps, thresholds, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_s = [ann_to_snn(model, thresholds, thresholds1, n_steps) for model in model_s]\n",
    "sd_s = [torch.load(f'{prefix}cifar10_vgg16_{ckpt_model_idx}_updated_snn1_1.pth', weights_only=True, map_location='cpu')] + \\\n",
    "        [torch.load(f'{prefix}cifar10_vgg16_{ckpt_model_idx}_updated_snn1_1_v{v}.pth', weights_only=True, map_location='cpu') for v in v_idx_s]\n",
    "for model, sd in zip(model_s, sd_s):\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 88.810000\n",
      "Accuracy of the network on the test images: 88.620000\n",
      "Accuracy of the network on the test images: 89.890000\n",
      "[88.81, 88.62] 89.89\n"
     ]
    }
   ],
   "source": [
    "# acc_s = [validate_snn(model, test_loader, n_steps, thresholds, device) for model in model_s]\n",
    "# acc_ensemble = validate_snn_ensemble(model_s, test_loader, n_steps, thresholds, device)\n",
    "# print(acc_s, acc_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 88.580000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.58"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_bn_stats(model_s[0], device, train_loader, n_steps, layerwise=False, ann=False)\n",
    "validate_snn(model_s[0], test_loader, n_steps, thresholds, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92.44 92.42 92.39 92.09 92.4  92.09 92.18 92.13 92.38 92.3  92.43 92.61\n",
      " 92.42 92.17 92.46 92.42 92.27 92.47 92.47 92.33 92.37 92.2  92.25 92.25\n",
      " 92.38 92.25 92.12 92.4  92.51 92.23] 94.36\n"
     ]
    }
   ],
   "source": [
    "# acc_list = np.array(acc_s + [acc_ensemble])\n",
    "# np.save('cifar10_vgg16_3_ft_acc_list.npy', acc_list)\n",
    "\n",
    "acc_list = np.load('cifar10_vgg16_3_ft_acc_list.npy')\n",
    "acc_s = acc_list[:-1]\n",
    "acc_ensemble = acc_list[-1]\n",
    "print(acc_s, acc_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92.44, 92.42, 92.39, 92.09, 92.4 , 92.09, 92.18, 92.13, 92.38,\n",
       "       92.3 , 92.43, 92.61, 92.42, 92.17, 92.46, 92.42, 92.27, 92.47,\n",
       "       92.47, 92.33, 92.37, 92.2 , 92.25, 92.25, 92.38, 92.25, 92.12,\n",
       "       92.4 , 92.51, 92.23])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92.61, 92.51, 92.47, 92.47, 92.46, 92.44, 92.43, 92.42, 92.42,\n",
       "       92.42])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_s[merged_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### uniform soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.380000\n",
      "linear acc: 92.38\n",
      "Accuracy of the network on the test images: 92.090000\n",
      "linear acc reset: 92.09\n",
      "Accuracy of the network on the test images: 92.530000\n",
      "slerp acc: 92.53\n",
      "Accuracy of the network on the test images: 92.320000\n",
      "slerp acc reset: 92.32\n"
     ]
    }
   ],
   "source": [
    "# bn_loader, _ = datapool(args.dataset, 512, 0, shuffle=False)\n",
    "model_mid = modelpool(args.model, args.dataset)\n",
    "model_mid = ann_to_snn(model_mid, thresholds, thresholds1, n_steps)\n",
    "merged_indices = range(0, num_models)\n",
    "\n",
    "sorted_indices = np.argsort(acc_s)[::-1]\n",
    "merged_indices = sorted_indices[:30]\n",
    "# merged_indices = [0, 2]\n",
    "\n",
    "merged_sds = [sd_s[i] for i in merged_indices]\n",
    "num_models = len(merged_sds)\n",
    "merged_weights = [1/num_models]*num_models\n",
    "\n",
    "sd_mid = interpolate_multi_state_dicts(merged_sds, merged_weights, use_slerp=False)\n",
    "model_mid.load_state_dict(sd_mid)\n",
    "model_mid = model_mid.to(device)\n",
    "acc_mid = validate_snn(model_mid, test_loader, n_steps, thresholds, device)\n",
    "print(f'linear acc: {acc_mid}')\n",
    "reset_bn_stats(model_mid, device, train_loader, n_steps, layerwise=False, ann=False)\n",
    "acc_mid_reset = validate_snn(model_mid, test_loader, n_steps, thresholds, device)\n",
    "print(f'linear acc reset: {acc_mid_reset}')\n",
    "\n",
    "sd_mid = interpolate_multi_state_dicts(merged_sds, merged_weights, use_slerp=True)\n",
    "model_mid.load_state_dict(sd_mid)\n",
    "model_mid = model_mid.to(device)\n",
    "acc_mid = validate_snn(model_mid, test_loader, n_steps, thresholds, device)\n",
    "print(f'slerp acc: {acc_mid}')\n",
    "reset_bn_stats(model_mid, device, train_loader, n_steps, layerwise=False, ann=False)\n",
    "acc_mid_reset = validate_snn(model_mid, test_loader, n_steps, thresholds, device)\n",
    "print(f'slerp acc reset: {acc_mid_reset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 89.980000\n"
     ]
    }
   ],
   "source": [
    "reset_bn_stats(model_mid, device, train_loader, n_steps, layerwise=False, ann=False)\n",
    "acc_mid_reset = validate_snn(model_mid, test_loader, n_steps, thresholds, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.360000\n",
      "slerp acc reset: 92.36\n"
     ]
    }
   ],
   "source": [
    "model_mid = deepcopy(model_s[0])\n",
    "sd_mid = interpolate_multi_state_dicts(merged_sds, merged_weights, use_slerp=True)\n",
    "model_mid.load_state_dict(sd_mid)\n",
    "model_mid = model_mid.to(device)\n",
    "# acc_mid = validate_snn(model_mid, test_loader, n_steps, thresholds, device)\n",
    "# print(f'slerp acc: {acc_mid}')\n",
    "reset_bn_stats(model_mid, device, train_loader, n_steps, layerwise=False, ann=False)\n",
    "acc_mid_reset = validate_snn(model_mid, test_loader, n_steps, thresholds, device)\n",
    "print(f'slerp acc reset: {acc_mid_reset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### greedy soup\n",
    "\n",
    "1. sort all models according to their test acc (descending)\n",
    "2. add the next best model to the soup\n",
    "3. remain the model only if it has a better test acc than the current soup\n",
    "4. repeat step 2 and 3 until no more models can be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_slerp: {use_slerp}\n",
      "test model 11\n",
      "add model 11, current acc: 92.61\n",
      "\n",
      "test model 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 92.19\n",
      "test model 10\n",
      "acc: 92.25\n",
      "test model 12\n",
      "acc: 92.18\n",
      "test model 1\n",
      "acc: 92.31\n",
      "test model 4\n",
      "acc: 92.11\n",
      "test model 2\n",
      "acc: 92.38\n",
      "test model 8\n",
      "acc: 92.37\n",
      "test model 9\n",
      "acc: 92.25\n",
      "test model 6\n",
      "acc: 92.13\n",
      "test model 13\n",
      "acc: 92.1\n",
      "test model 7\n",
      "acc: 92.25\n",
      "test model 5\n",
      "acc: 92.13\n",
      "test model 3\n",
      "acc: 92.31\n"
     ]
    }
   ],
   "source": [
    "def grid_search(model_1, model_2, grid, use_slerp=False):\n",
    "    t_best = None\n",
    "    acc_best = -1\n",
    "    for t in grid:\n",
    "        sd_mid = interpolate_state_dicts(model_1.state_dict(), model_2.state_dict(), t, use_slerp=use_slerp)\n",
    "        model_mid.load_state_dict(sd_mid)\n",
    "        acc_mid = validate_snn(model_mid, test_loader, n_steps, thresholds, device, verbose=0)\n",
    "        if acc_mid > acc_best:\n",
    "            acc_best = acc_mid\n",
    "            t_best = t\n",
    "    return sd_mid, acc_best, t_best\n",
    "        \n",
    "\n",
    "# 1: sort indices by acc_s in descending order\n",
    "sorted_indices = np.argsort(acc_s)[::-1]\n",
    "acc_cur = -1\n",
    "model_mid = None\n",
    "num_merged = 0\n",
    "merged_indices = []\n",
    "use_slerp = True\n",
    "print(f\"use_slerp: {use_slerp}\")\n",
    "for i in sorted_indices:\n",
    "    print(f\"test model {i}\")\n",
    "    if model_mid is None:\n",
    "        model_mid = deepcopy(model_s[i])\n",
    "        acc_cur = acc_s[i]\n",
    "        print(f\"add model {i}, current acc: {acc_cur}\" + \"\\n\")\n",
    "        num_merged += 1\n",
    "        merged_indices.append(i)\n",
    "    else:\n",
    "        sd_mid_ori = deepcopy(model_mid.state_dict())\n",
    "        t = 1 / (num_merged + 1)\n",
    "        sd_mid = interpolate_state_dicts(model_mid.state_dict(), model_s[i].state_dict(), t, use_slerp=use_slerp)\n",
    "        model_mid.load_state_dict(sd_mid)\n",
    "        acc_mid = validate_snn(model_mid, test_loader, n_steps, thresholds, device, verbose=0)\n",
    "        print(f'acc: {acc_mid}')\n",
    "        # reset_bn_stats(model_mid, device, train_loader, n_steps, layerwise=False, ann=False)\n",
    "        # acc_mid_reset = validate_snn(model_mid, test_loader, n_steps, thresholds, device, verbose=0)\n",
    "        # print(f'reset acc: {acc_mid_reset}')\n",
    "\n",
    "        # if acc_mid > acc_mid_reset:\n",
    "        #     model_mid.load_state_dict(sd_mid)\n",
    "        #     acc_best = acc_mid\n",
    "        # else:\n",
    "        #     acc_best = acc_mid_reset\n",
    "        if acc_mid > acc_cur: # use acc_mid for comparison or acc_best\n",
    "            acc_cur = acc_mid\n",
    "            print(f\"add model {i}, current acc: {acc_cur}\" + \"\\n\")\n",
    "            num_merged += 1\n",
    "            merged_indices.append(i)\n",
    "        else:\n",
    "            print('\\n')\n",
    "            model_mid.load_state_dict(sd_mid_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.350000\n",
      "Accuracy of the network on the test images: 92.360000\n",
      "Accuracy of the network on the test images: 92.530000\n",
      "Accuracy of the network on the test images: 92.180000\n",
      "Accuracy of the network on the test images: 92.220000\n",
      "Accuracy of the network on the test images: 92.540000\n",
      "Accuracy of the network on the test images: 92.460000\n",
      "Accuracy of the network on the test images: 92.360000\n",
      "Accuracy of the network on the test images: 92.360000\n",
      "Accuracy of the network on the test images: 92.370000\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    reset_bn_stats(model_mid, device, train_loader, n_steps, layerwise=False, ann=False)\n",
    "    validate_snn(model_mid, test_loader, n_steps, thresholds, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grid search on t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  0, 10, 12,  1,  4,  2,  8,  9,  6, 13,  7,  5,  3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[92.44,\n",
       " 92.42,\n",
       " 92.39,\n",
       " 92.09,\n",
       " 92.4,\n",
       " 92.09,\n",
       " 92.18,\n",
       " 92.13,\n",
       " 92.38,\n",
       " 92.3,\n",
       " 92.43,\n",
       " 92.61,\n",
       " 92.42,\n",
       " 92.17]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 90.010000\n",
      "Accuracy of the network on the test images: 89.870000\n",
      "Accuracy of the network on the test images: 89.810000\n",
      "Accuracy of the network on the test images: 89.930000\n",
      "Accuracy of the network on the test images: 90.310000\n",
      "Accuracy of the network on the test images: 89.900000\n",
      "Accuracy of the network on the test images: 90.160000\n",
      "Accuracy of the network on the test images: 89.950000\n",
      "Accuracy of the network on the test images: 90.100000\n",
      "Accuracy of the network on the test images: 89.940000\n",
      "Accuracy of the network on the test images: 89.790000\n",
      "Accuracy of the network on the test images: 89.980000\n",
      "Accuracy of the network on the test images: 90.270000\n",
      "Accuracy of the network on the test images: 90.130000\n",
      "Accuracy of the network on the test images: 89.920000\n",
      "Accuracy of the network on the test images: 90.060000\n",
      "Accuracy of the network on the test images: 89.710000\n",
      "Accuracy of the network on the test images: 90.000000\n",
      "Accuracy of the network on the test images: 89.740000\n",
      "Accuracy of the network on the test images: 90.040000\n"
     ]
    }
   ],
   "source": [
    "# two models\n",
    "merged_indices = [0, 1]\n",
    "merged_sds = [sd_s[i] for i in merged_indices]\n",
    "t_s = np.linspace(0.52, 0.53, 20)\n",
    "acc_t_s = []\n",
    "for t in t_s:\n",
    "    merged_weights = [t, 1-t]\n",
    "    sd_mid = interpolate_multi_state_dicts(merged_sds, merged_weights, use_slerp=True)\n",
    "    # sd_mid = interpolate_state_dicts(merged_sds[0], merged_sds[1], 0.5, use_slerp=True)\n",
    "    model_mid.load_state_dict(sd_mid)\n",
    "    model_mid = model_mid.to(device)\n",
    "    acc_mid = validate_snn(model_mid, test_loader, n_steps, thresholds, device)\n",
    "    acc_t_s.append(acc_mid)\n",
    "t_acc_dict = {t: acc for t, acc in zip(t_s, acc_t_s)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.4: 92.03,\n",
       " 0.4105263157894737: 92.22,\n",
       " 0.4210526315789474: 92.17,\n",
       " 0.43157894736842106: 92.22,\n",
       " 0.4421052631578948: 92.16,\n",
       " 0.45263157894736844: 92.24,\n",
       " 0.4631578947368421: 92.31,\n",
       " 0.4736842105263158: 92.2,\n",
       " 0.4842105263157895: 92.02,\n",
       " 0.49473684210526314: 92.25,\n",
       " 0.5052631578947369: 92.19,\n",
       " 0.5157894736842106: 92.13,\n",
       " 0.5263157894736842: 92.16,\n",
       " 0.5368421052631579: 92.48,\n",
       " 0.5473684210526315: 92.04,\n",
       " 0.5578947368421052: 92.16,\n",
       " 0.5684210526315789: 92.18,\n",
       " 0.5789473684210527: 92.27,\n",
       " 0.5894736842105264: 92.47,\n",
       " 0.6: 92.15}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 91.890000\n"
     ]
    }
   ],
   "source": [
    "t = 0.5\n",
    "merged_weights = [t, 1-t]\n",
    "sd_mid = interpolate_multi_state_dicts(merged_sds, merged_weights, use_slerp=True)\n",
    "# sd_mid = interpolate_state_dicts(merged_sds[0], merged_sds[1], 0.5, use_slerp=True)\n",
    "model_mid.load_state_dict(sd_mid)\n",
    "model_mid = model_mid.to(device)\n",
    "acc_mid = validate_snn(model_mid, test_loader, n_steps, thresholds, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reset bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.310000\n",
      "Accuracy of the network on the test images: 92.200000\n",
      "Accuracy of the network on the test images: 92.360000\n",
      "Accuracy of the network on the test images: 92.280000\n",
      "Accuracy of the network on the test images: 92.140000\n",
      "Accuracy of the network on the test images: 92.240000\n",
      "Accuracy of the network on the test images: 92.120000\n",
      "Accuracy of the network on the test images: 91.870000\n",
      "Accuracy of the network on the test images: 92.560000\n",
      "Accuracy of the network on the test images: 92.580000\n"
     ]
    }
   ],
   "source": [
    "reset_acc_s = []\n",
    "# bn_loader, _ = datapool(args.dataset, 128, 0, shuffle=True)\n",
    "for _ in range(10):\n",
    "    bn_loader = train_loader\n",
    "    model_mid.load_state_dict(sd_mid)\n",
    "    reset_bn_stats(model_mid, device, bn_loader, n_steps, layerwise=False)\n",
    "    reset_acc_mid = validate_snn(model_mid, test_loader, n_steps, thresholds, device)\n",
    "    # print(\"Training acc\", validate_snn(model_mid, bn_loader, n_steps, thresholds, device))\n",
    "    reset_acc_s.append(reset_acc_mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.68"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(reset_acc_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try fine-tuning to tune bn statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_snn(train_dataloader, test_dataloader, model, epochs, device, loss_fn, lr=0.1, wd=5e-4):\n",
    "#     model = model.to(device)\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=lr,momentum=0.9,weight_decay=wd) \n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs,verbose=True)\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0\n",
    "#         length = 0\n",
    "#         total = 0\n",
    "#         correct = 0\n",
    "#         for img, label in train_dataloader:\n",
    "#             img = add_dimension(img,n_steps)\n",
    "#             img = img.cuda(device)\n",
    "            \n",
    "#             labels = label.cuda(device)\n",
    "#             outputs = model(img,thresholds,L=0,t=n_steps) \n",
    "#             outputs = torch.sum(outputs,1)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = loss_fn(outputs/n_steps, labels)\n",
    "\n",
    "#             with torch.autograd.set_detect_anomaly(True):\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "                \n",
    "#             epoch_loss += loss.item()*img.shape[0]\n",
    "#             length += len(label)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#             #print(predicted)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             if total%(256*8) == 0:\n",
    "#                 print('Epoch:%d, Accuracy of the snn network on the %d train images: %f, loss:%f'%(epoch,total,100 * correct / total,epoch_loss/total))\n",
    "#         print('Epoch:%d, Accuracy of the snn network on the %d train images: %f, loss:%f'%(epoch,total,100 * correct / total,epoch_loss/total))\n",
    "#         scheduler.step()\n",
    "#         validate_snn(model, test_dataloader, n_steps, thresholds, device)\n",
    "# model_mid.load_state_dict(sd_mid) \n",
    "# train_snn(train_loader, test_loader, model_mid, 5, device, criterion, lr=1e-5, wd=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = np.load('cifar10_vgg16_0_threshold_all_noaug2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.load('logs/cifar10_vgg16_3_updated_snn1_test_acc_1_v22.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(91.95)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
